Metadata-Version: 2.2
Name: visrl
Version: 0.9.2.dev0
Summary: VisRL repo changed from LLama-Factory
Home-page: https://github.com/zhangquanchen/VisRL
Author: zhangquan
Author-email: czq23@mails.tsinghua.edu.cn
License: Apache 2.0 License
Keywords: LLM,RL,transformer,pytorch,deep learning
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8.0
Description-Content-Type: text/markdown
Requires-Dist: transformers<=4.46.1,>=4.41.2
Requires-Dist: datasets<=3.1.0,>=2.16.0
Requires-Dist: accelerate<=1.0.1,>=0.34.0
Requires-Dist: peft<=0.12.0,>=0.11.1
Requires-Dist: trl<=0.9.6,>=0.8.6
Requires-Dist: tokenizers<0.20.4,>=0.19.0
Requires-Dist: gradio<5.0.0,>=4.0.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: scipy
Requires-Dist: einops
Requires-Dist: sentencepiece
Requires-Dist: tiktoken
Requires-Dist: protobuf
Requires-Dist: uvicorn
Requires-Dist: pydantic
Requires-Dist: fastapi
Requires-Dist: sse-starlette
Requires-Dist: matplotlib>=3.7.0
Requires-Dist: fire
Requires-Dist: packaging
Requires-Dist: pyyaml
Requires-Dist: numpy<2.0.0
Requires-Dist: av
Requires-Dist: tyro<0.9.0
Provides-Extra: torch
Requires-Dist: torch>=1.13.1; extra == "torch"
Provides-Extra: torch-npu
Requires-Dist: torch==2.1.0; extra == "torch-npu"
Requires-Dist: torch-npu==2.1.0.post3; extra == "torch-npu"
Requires-Dist: decorator; extra == "torch-npu"
Provides-Extra: metrics
Requires-Dist: nltk; extra == "metrics"
Requires-Dist: jieba; extra == "metrics"
Requires-Dist: rouge-chinese; extra == "metrics"
Provides-Extra: deepspeed
Requires-Dist: deepspeed<=0.14.4,>=0.10.0; extra == "deepspeed"
Provides-Extra: liger-kernel
Requires-Dist: liger-kernel; extra == "liger-kernel"
Provides-Extra: bitsandbytes
Requires-Dist: bitsandbytes>=0.39.0; extra == "bitsandbytes"
Provides-Extra: hqq
Requires-Dist: hqq; extra == "hqq"
Provides-Extra: eetq
Requires-Dist: eetq; extra == "eetq"
Provides-Extra: gptq
Requires-Dist: optimum>=1.17.0; extra == "gptq"
Requires-Dist: auto-gptq>=0.5.0; extra == "gptq"
Provides-Extra: awq
Requires-Dist: autoawq; extra == "awq"
Provides-Extra: aqlm
Requires-Dist: aqlm[gpu]>=1.1.0; extra == "aqlm"
Provides-Extra: vllm
Requires-Dist: vllm<0.6.5,>=0.4.3; extra == "vllm"
Provides-Extra: galore
Requires-Dist: galore-torch; extra == "galore"
Provides-Extra: badam
Requires-Dist: badam>=1.2.1; extra == "badam"
Provides-Extra: adam-mini
Requires-Dist: adam-mini; extra == "adam-mini"
Provides-Extra: qwen
Requires-Dist: transformers_stream_generator; extra == "qwen"
Provides-Extra: modelscope
Requires-Dist: modelscope; extra == "modelscope"
Provides-Extra: openmind
Requires-Dist: openmind; extra == "openmind"
Provides-Extra: swanlab
Requires-Dist: swanlab; extra == "swanlab"
Provides-Extra: dev
Requires-Dist: pre-commit; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# VirRL: Intention-Driven Visual Perception via Reinforced Reasoning
[![arXiv](https://img.shields.io/badge/arXiv-PDF-red)](https://arxiv.org/pdf/2408.08568)
[![Google Drive](https://img.shields.io/badge/Dataset-GoogleDrive-yellow)](https://drive.google.com/drive/folders/1CK9qihI2yyxkuXsxSHqzTRhLdTk8qghn?usp=sharing)

## Instruction
Visual understanding is inherently intention-driven—humans selectively focus on different regions of a scene based on their goals. Recent advances in large multimodal models (LMMs) enable flexible expression of such intentions through natural language, allowing queries to guide visual reasoning processes. Frameworks like Visual Chain-of-Thought have demonstrated the benefit of incorporating explicit reasoning steps, where the model predicts a focus region before answering a query. However, existing approaches rely heavily on supervised training with annotated intermediate bounding boxes, which severely limits scalability due to the combinatorial explosion of intention-region pairs. To overcome this limitation, we propose VisRL, the first framework that applies reinforcement learning (RL) to the problem of intention-driven visual perception. VisRL optimizes the entire visual reasoning process using only reward signals. By treating intermediate focus selection as a internal decision optimized through trial-and-error, our method eliminates the need for costly region annotations while aligning more closely with how humans learn to perceive the world. Extensive experiments across multiple benchmarks show that VisRL consistently outperforms strong baselines, demonstrating both its effectiveness and its strong generalization across different LMMs.

## Method overview
<img src="assets/pipeline.png" alt="drawing" width="500"/>

## Env Setup
```bash
cd VisRL
conda create -n visrl python=3.10
pip install -e ".[torch,metrics]"
```

## SFT
```bash
nohup visrl-cli train examples/train_lora/qwen2vl_lora_sft.yaml > output_14w.log 2>&1 &
visrl-cli export examples/merge_lora/qwen2vl_lora_sft.yaml
```

## Inference
```bash
API_PORT=8000 visrl-cli api examples/inference/qwen2_vl.yaml
python scripts/api_example/test.py
```

## 服务器传输
```
cd /datadisk/zhangquan/azcopy_linux_amd64_10.27.1/
azcopy copy "/datadisk/zhangquan/LLaMA-Factory/models/qwen2_vl-7b_viscot_lora_sft_round2_1w" "https://xufluo.blob.core.windows.net/zhangquanchen/models_tmp?sv=2023-01-03&st=2025-01-13T11%3A30%3A40Z&se=2025-01-20T11%3A30%3A00Z&skoid=e1812c39-c78d-4fc2-95dd-7d7fbee82af4&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2025-01-13T11%3A30%3A40Z&ske=2025-01-20T11%3A30%3A00Z&sks=b&skv=2023-01-03&sr=c&sp=racwdxltf&sig=EShhtimZVG9iOlXC0Q%2FzjIo%2BCQoM7SJ6gW0RDv6kowc%3D" --recursive
```

## DPO
```
conda activate lf-sft
visrl-cli train examples/train_lora/qwen2vl_lora_dpo.yaml
```

## Acknowledgement
This repo is changed from [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory.git). Thanks for their wonderful works.
